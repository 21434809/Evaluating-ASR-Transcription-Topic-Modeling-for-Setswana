{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Step 1: Load Wordlist (skipping first 25 lines, taking column 2 only)\n",
        "def load_wordlist(filepath, skip_lines=25):\n",
        "    wordlist = set()\n",
        "    try:\n",
        "        with open(filepath, encoding='utf-8') as f:\n",
        "            for idx, line in enumerate(f):\n",
        "                if idx < skip_lines:\n",
        "                    continue  # skip initial lines\n",
        "                parts = line.strip().split('\\t')\n",
        "                if len(parts) >= 2:\n",
        "                    word = parts[1].strip().lower()\n",
        "                    if word.isalpha():  # Only add alphabetic words\n",
        "                        wordlist.add(word)\n",
        "        print(f\"Loaded {len(wordlist)} words from {filepath}.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Wordlist file not found at {filepath}\")\n",
        "    return wordlist\n",
        "\n",
        "# Step 2: Word Segmentation Function (Greedy Max Matching)\n",
        "def segment_word(text, wordlist):\n",
        "    \"\"\"\n",
        "    Segments a string into words using the Maximum Matching algorithm with a given wordlist.\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    i = 0\n",
        "    result = []\n",
        "    while i < len(text):\n",
        "        match = None\n",
        "        # Check from longest possible match to shortest\n",
        "        for j in range(len(text), i, -1):\n",
        "            word = text[i:j]\n",
        "            if word in wordlist:\n",
        "                match = word\n",
        "                result.append(match)\n",
        "                i += len(word)\n",
        "                break\n",
        "        if not match:\n",
        "            # No match found, treat as single char or unknown\n",
        "            result.append(text[i])\n",
        "            i += 1\n",
        "    return ' '.join(result)\n",
        "\n",
        "# Example Usage (similar to the __main__ block in other files)\n",
        "if __name__ == \"__main__\":\n",
        "    wordlist_file = 'tsn-za_web_2020_10K-words.txt'\n",
        "    wordlist = load_wordlist(wordlist_file)\n",
        "\n",
        "    input_filename = \"podcast_transcriptions_chunked_sorted_numbered+english.csv\"\n",
        "    output_csv_file = \"podcast_transcriptions_chunked_sents_segmented.csv\"\n",
        "\n",
        "    # Load the input CSV\n",
        "    try:\n",
        "        df = pd.read_csv(input_filename)\n",
        "        print(f\"Loaded data from {input_filename}.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Input CSV file not found at {input_filename}\")\n",
        "        exit()\n",
        "\n",
        "    # Process each row in the 'transcription' column with the segment_word function\n",
        "    # Ensure the 'transcription' column exists and handle potential NaN values\n",
        "    if 'transcription' in df.columns:\n",
        "        df['Segmented_Transcription'] = df['transcription'].apply(\n",
        "            lambda x: segment_word(x, wordlist) if pd.notnull(x) else ''\n",
        "        )\n",
        "        print(\"Segmentation applied to 'transcription' column.\")\n",
        "    else:\n",
        "        print(\"Error: 'transcription' column not found in the input CSV.\")\n",
        "        exit()\n",
        "\n",
        "\n",
        "    # Save the processed DataFrame to a new CSV file\n",
        "    df.to_csv(output_csv_file, index=False)\n",
        "    print(f\"Segmented data saved to {output_csv_file}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiF2kv3gDbAB",
        "outputId": "3f7992f0-a9fe-4e34-cffc-60f6f1f99b2d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 17452 words from tsn-za_web_2020_10K-words.txt.\n",
            "Loaded data from podcast_transcriptions_chunked_sorted_numbered+english.csv.\n",
            "Segmentation applied to 'transcription' column.\n",
            "Segmented data saved to podcast_transcriptions_chunked_sents_segmented.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tV3_kk5XG17D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4F96JPRjJpa",
        "outputId": "cc915b4d-984a-4e8f-b6ed-0d9b027c0337"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed data saved to lemmatized_transcriptions.csv\n",
            "\n",
            "Test Cases:\n",
            "Setswana Verb Lemmatizer Test Cases\n",
            "========================================\n",
            "supiwa          → supa\n",
            "supisa          → supa\n",
            "supisisa        → supa\n",
            "supela          → supa\n",
            "supana          → supa\n",
            "ikopa           → opa\n",
            "iphenya         → bhenya\n",
            "robakaka        → roba\n",
            "bofolola        → bofa\n",
            "rapelang        → rapa\n",
            "itshupile       → lshupa\n",
            "mmetsa          → betsa\n",
            "mpona           → bona\n",
            "palame          → palama\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "class SetswanaLemmatizer:\n",
        "    def __init__(self):\n",
        "        # Initialize lookup tables for exceptions\n",
        "        self.passive_exceptions = {'ungwa', 'wa', 'swa', 'nwa', 'hwa'}\n",
        "        self.causative_exceptions = {'tataisa', 'gaisa', 'laisa', 'fisa'}\n",
        "        self.applicative_exceptions = {'bela', 'sela', 'tlhatlhela'}\n",
        "        self.reciprocal_exceptions = {'pana', 'gana', 'fapaana', 'rulagana'}\n",
        "        self.neuter_passive_exceptions = {'sega', 'bega', 'anega', 'pega'}\n",
        "\n",
        "        # Define transformation rules\n",
        "        self.transformations = [\n",
        "            self._remove_plural,\n",
        "            self._remove_perfect_tense,\n",
        "            self._remove_passive,\n",
        "            self._remove_reciprocal,\n",
        "            self._remove_applicative,\n",
        "            self._remove_neuter_passive,\n",
        "            self._remove_causative,\n",
        "            self._remove_reversal,\n",
        "            self._remove_reflexive,\n",
        "            self._remove_object_markers,\n",
        "            self._remove_iterative,\n",
        "            self._fix_mood\n",
        "        ]\n",
        "\n",
        "        # Define perfect tense conversions\n",
        "        self.perfect_conversions = {\n",
        "            'etswe': 'lwa', 'otswe': 'lwa', 'utswe': 'lwa',\n",
        "            'ditse': 'tsa', 'tswitse': 'tsa',\n",
        "            'elle': 'aa',\n",
        "            'ntse': 'nya',\n",
        "            'tshtswe': 'tshwa',\n",
        "            'sitswe': 'shwa', 'sitswe': 'swa',\n",
        "            'tshtse': 'tsha',\n",
        "            'ntswe': 'mapwa',\n",
        "            'sitse': 'sa',\n",
        "            'dile': 'la', 'tse': 'la',\n",
        "            'lwe': 'wa',\n",
        "            'ditswe': 'tswa', 'tswitswe': 'tswa', 'tsitswe': 'tswa',\n",
        "            'ile': 'a',\n",
        "            'nne': 'na',\n",
        "            'nwe': 'nwa'\n",
        "        }\n",
        "\n",
        "        # Define reflexive transformations\n",
        "        self.reflexive_transforms = {\n",
        "            'a': 'ika',\n",
        "            'e': 'ike',\n",
        "            'i': 'iki',\n",
        "            'o': 'iko',\n",
        "            'u': 'iku',\n",
        "            'w': 'ikw',\n",
        "            'g': 'ikg',\n",
        "            'b': 'ip',\n",
        "            'l': 'it',\n",
        "            'r': 'ith',\n",
        "            's': 'itsh',\n",
        "            'd': 'it',\n",
        "            'h': 'iph',  # simplified - paper mentions more complex cases\n",
        "            'f': 'iph'\n",
        "        }\n",
        "\n",
        "    def lemmatize(self, word):\n",
        "        \"\"\"\n",
        "        Lemmatize a Setswana verb by applying transformation rules in sequence\n",
        "        \"\"\"\n",
        "        original_word = word\n",
        "        changed = True\n",
        "\n",
        "        # Apply transformations until no more changes occur\n",
        "        while changed:\n",
        "            changed = False\n",
        "            for transform in self.transformations:\n",
        "                new_word = transform(word)\n",
        "                if new_word != word:\n",
        "                    word = new_word\n",
        "                    changed = True\n",
        "                    break  # restart transformations after each change\n",
        "\n",
        "        return word if word != original_word else original_word\n",
        "\n",
        "    def _remove_plural(self, word):\n",
        "        \"\"\"Remove plural suffix -ng\"\"\"\n",
        "        if word.endswith('ng'):\n",
        "            return word[:-2]\n",
        "        return word\n",
        "\n",
        "    def _remove_perfect_tense(self, word):\n",
        "        \"\"\"Remove perfect tense suffixes\"\"\"\n",
        "        if word in self.passive_exceptions:\n",
        "            return word\n",
        "\n",
        "        for suffix, replacement in self.perfect_conversions.items():\n",
        "            if word.endswith(suffix):\n",
        "                return word[:-len(suffix)] + replacement\n",
        "\n",
        "        # Special case for -ile (most common perfect tense)\n",
        "        if word.endswith('ile'):\n",
        "            return word[:-3] + 'a'\n",
        "\n",
        "        return word\n",
        "\n",
        "    def _remove_passive(self, word):\n",
        "        \"\"\"Remove passive suffixes\"\"\"\n",
        "        if word in self.passive_exceptions:\n",
        "            return word\n",
        "\n",
        "        # Table I transformations from the paper\n",
        "        passive_transforms = {\n",
        "            'biwa': 'ba', 'jwa': 'ba',\n",
        "            'fiwa': 'fa', 'swa': 'fa',\n",
        "            'giwa': 'ga', 'gwa': 'ga',\n",
        "            'piwa': 'pa', 'tswa': 'pa',\n",
        "            'miwa': 'ma', 'ngwa': 'ma',\n",
        "            'niwa': 'na', 'nwa': 'na',\n",
        "            'nyiwa': 'nya', 'nywa': 'nya',\n",
        "            'diwa': 'tsa', 'tswa': 'tsa',\n",
        "            'tliwa': 'tlha', 'tlhwa': 'tlha',\n",
        "            'tliwa': 'tla', 'tlhwa': 'tla',\n",
        "            'tiwa': 'ta', 'twa': 'ta',\n",
        "            'siwa': 'sa', 'swa': 'sa',\n",
        "            'wiwa': 'wa',\n",
        "            'wa': 'a'\n",
        "        }\n",
        "\n",
        "        for suffix, replacement in passive_transforms.items():\n",
        "            if word.endswith(suffix):\n",
        "                return word[:-len(suffix)] + replacement\n",
        "\n",
        "        return word\n",
        "\n",
        "    def _remove_causative(self, word):\n",
        "        \"\"\"Remove causative suffix -is-\"\"\"\n",
        "        if word in self.causative_exceptions:\n",
        "            return word\n",
        "\n",
        "        if word.endswith('isha'):\n",
        "            return word[:-4] + 'a'\n",
        "        elif word.endswith('isa'):\n",
        "            return word[:-3] + 'a'\n",
        "        elif word.endswith('isisa'):  # intensity form\n",
        "            return word[:-5] + 'a'\n",
        "\n",
        "        return word\n",
        "\n",
        "    def _remove_applicative(self, word):\n",
        "        \"\"\"Remove applicative suffix -el-\"\"\"\n",
        "        if word in self.applicative_exceptions:\n",
        "            return word\n",
        "\n",
        "        if word.endswith('ela'):\n",
        "            return word[:-3] + 'a'\n",
        "        elif word.endswith('ele'):\n",
        "            return word[:-3] + 'a'\n",
        "        elif word.endswith('elwa'):\n",
        "            return word[:-4] + 'a'\n",
        "\n",
        "        return word\n",
        "\n",
        "    def _remove_reciprocal(self, word):\n",
        "        \"\"\"Remove reciprocal suffix -an-\"\"\"\n",
        "        if word in self.reciprocal_exceptions:\n",
        "            return word\n",
        "\n",
        "        if word.endswith('ana'):\n",
        "            return word[:-3] + 'a'\n",
        "        elif word.endswith('anya'):\n",
        "            return word[:-4] + 'a'\n",
        "\n",
        "        return word\n",
        "\n",
        "    def _remove_neuter_passive(self, word):\n",
        "        \"\"\"Remove neuter-passive suffixes (-eg-, -al-, -agal-, -eseg-)\"\"\"\n",
        "        if word in self.neuter_passive_exceptions:\n",
        "            return word\n",
        "\n",
        "        if word.endswith('ega'):\n",
        "            return word[:-3] + 'a'\n",
        "        elif word.endswith('ala'):\n",
        "            return word[:-3] + 'a'\n",
        "        elif word.endswith('agala'):\n",
        "            return word[:-5] + 'a'\n",
        "        elif word.endswith('esega'):\n",
        "            return word[:-5] + 'a'\n",
        "\n",
        "        return word\n",
        "\n",
        "    def _remove_reversal(self, word):\n",
        "        \"\"\"Remove reversal suffix -olol-\"\"\"\n",
        "        # As noted in the paper, most words with -olol- are basic forms\n",
        "        # So we only handle specific cases that we know need transformation\n",
        "        reversal_examples = {\n",
        "            'bofolola': 'bofa',\n",
        "            'kopolola': 'kopa'\n",
        "        }\n",
        "\n",
        "        return reversal_examples.get(word, word)\n",
        "\n",
        "    def _remove_iterative(self, word):\n",
        "        \"\"\"Remove iterative suffix -ka-\"\"\"\n",
        "        if 'kaka' in word:\n",
        "            return re.sub(r'kaka$', '', word)\n",
        "        elif word.endswith('ka'):\n",
        "            return word[:-2]\n",
        "        return word\n",
        "\n",
        "    def _remove_reflexive(self, word):\n",
        "        \"\"\"Remove reflexive prefix i- with transformations\"\"\"\n",
        "        if not word.startswith('i'):\n",
        "            return word\n",
        "\n",
        "        # Handle reflexive transformations from Table II\n",
        "        for initial, prefix in self.reflexive_transforms.items():\n",
        "            if word.startswith(prefix):\n",
        "                # Remove the reflexive prefix and restore the original initial\n",
        "                return initial + word[len(prefix):]\n",
        "\n",
        "        # Simple case: just remove 'i' prefix\n",
        "        if word.startswith('i'):\n",
        "            return word[1:]\n",
        "\n",
        "        return word\n",
        "\n",
        "    def _remove_object_markers(self, word):\n",
        "        \"\"\"Remove object markers (first-person n-, third-person mo-)\"\"\"\n",
        "        # First-person object marker n- becomes m- before certain consonants\n",
        "        if word.startswith('m') and len(word) > 1:\n",
        "            next_char = word[1]\n",
        "            if next_char in {'p', 'b', 'ph', 'f'}:\n",
        "                return 'b' + word[2:]  # n- becomes m- and original consonant was b/p/ph/f\n",
        "\n",
        "        # Third-person object marker mo- contracted to m- and b- becomes -m\n",
        "        if word.startswith('mm') and len(word) > 2:\n",
        "            return 'b' + word[2:]  # e.g., mmetsa -> beta\n",
        "\n",
        "        if word.startswith('n'):\n",
        "            return word[1:]\n",
        "\n",
        "        if word.startswith('mo'):\n",
        "            return word[2:]\n",
        "\n",
        "        return word\n",
        "\n",
        "    def _fix_mood(self, word):\n",
        "        \"\"\"Fix mood by replacing -e with -a\"\"\"\n",
        "        if word.endswith('e'):\n",
        "            return word[:-1] + 'a'\n",
        "        return word\n",
        "\n",
        "\n",
        "def process_transcription(transcription, lemmatizer):\n",
        "    \"\"\"\n",
        "    Process a transcription string with numbered sentences\n",
        "    Returns a new string with lemmatized words\n",
        "    \"\"\"\n",
        "    # Split into numbered sentences\n",
        "    sentences = [s.strip() for s in transcription.split('\\n') if s.strip()]\n",
        "\n",
        "    processed_sentences = []\n",
        "    for sentence in sentences:\n",
        "        # Split into number and text (e.g., \"1. e kitshedimosetso...\")\n",
        "        parts = sentence.split('.', 1)\n",
        "        if len(parts) == 2:\n",
        "            num_part, text_part = parts\n",
        "            # Lemmatize each word in the text\n",
        "            words = text_part.split()\n",
        "            lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "            # Reconstruct the sentence\n",
        "            processed_sentence = f\"{num_part}. {' '.join(lemmatized_words)}\"\n",
        "            processed_sentences.append(processed_sentence)\n",
        "        else:\n",
        "            # If no number, just process the text\n",
        "            words = sentence.split()\n",
        "            lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "            processed_sentences.append(' '.join(lemmatized_words))\n",
        "\n",
        "    return '\\n'.join(processed_sentences)\n",
        "\n",
        "\n",
        "def process_csv(input_file, output_file):\n",
        "    \"\"\"\n",
        "    Process the CSV file, lemmatizing the transcription column\n",
        "    \"\"\"\n",
        "    # Read the CSV file\n",
        "    df = pd.read_csv(input_file)\n",
        "\n",
        "    # Initialize lemmatizer\n",
        "    lemmatizer = SetswanaLemmatizer()\n",
        "\n",
        "    # Process each transcription\n",
        "    df['Lemmatized_Transcription'] = df['transcription'].apply(\n",
        "        lambda x: process_transcription(x, lemmatizer) if pd.notnull(x) else ''\n",
        "    )\n",
        "\n",
        "    # Save to new file\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"Processed data saved to {output_file}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage with the podcast transcriptions file\n",
        "    input_filename = \"podcast_transcriptions_chunked_sents_segmented.csv\"\n",
        "    output_filename = \"lemmatized_transcriptions.csv\"\n",
        "\n",
        "    process_csv(input_filename, output_filename)\n",
        "\n",
        "    print(\"\\nTest Cases:\")\n",
        "    lemmatizer = SetswanaLemmatizer()\n",
        "    test_words = [\n",
        "        'supiwa',    # passive of supa (point)\n",
        "        'supisa',    # causative of supa\n",
        "        'supisisa',  # intensity of supa\n",
        "        'supela',    # applicative of supa\n",
        "        'supana',    # reciprocal of supa\n",
        "        'ikopa',     # reflexive of kopa (ask)\n",
        "        'iphenya',   # reflexive of fenya (win)\n",
        "        'robakaka',  # iterative of roba (break)\n",
        "        'bofolola',  # reversal of bofa (tie)\n",
        "        'rapelang',  # plural of rapela (pray)\n",
        "        'itshupile', # perfect reflexive of supa\n",
        "        'mmetsa',    # third-person object marker of beta (ask)\n",
        "        'mpona',     # first-person object marker of bona (see)\n",
        "        'palame'     # mood form of palama (climb)\n",
        "    ]\n",
        "\n",
        "    print(\"Setswana Verb Lemmatizer Test Cases\")\n",
        "    print(\"=\" * 40)\n",
        "    for word in test_words:\n",
        "        lemma = lemmatizer.lemmatize(word)\n",
        "        print(f\"{word:15} → {lemma}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(input_file, output_file, stopwords_file, num_stopwords=100):\n",
        "    \"\"\"\n",
        "    Loads the processed CSV, removes top N stopwords from a file,\n",
        "    and saves the result to a new CSV.\n",
        "\n",
        "    Args:\n",
        "        input_file (str): Path to the input CSV file (e.g., lemmatized_transcriptions.csv).\n",
        "        output_file (str): Path to save the output CSV file.\n",
        "        stopwords_file (str): Path to the stopwords text file (tswana_stopwords.txt).\n",
        "        num_stopwords (int): The number of top stopwords to load from the file.\n",
        "    \"\"\"\n",
        "    # Load the processed CSV\n",
        "    df = pd.read_csv(input_file)\n",
        "\n",
        "    # Load top N stopwords from the text file\n",
        "    try:\n",
        "        with open(stopwords_file, 'r', encoding='utf-8') as f:\n",
        "            stopwords = [line.strip() for line in f][:num_stopwords]\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Stopwords file not found at {stopwords_file}\")\n",
        "        return\n",
        "\n",
        "    # Function to remove stopwords from a single string\n",
        "    def remove_stopwords_from_text(text):\n",
        "        if pd.isnull(text):\n",
        "            return \"\"\n",
        "        words = text.split()\n",
        "        filtered_words = [word for word in words if word.lower() not in stopwords]\n",
        "        return ' '.join(filtered_words)\n",
        "\n",
        "    # Apply stop word removal to the lemmatized transcription column\n",
        "    df['Lemmatized_Transcription_Normal'] = df['Lemmatized_Transcription'].apply(remove_stopwords_from_text)\n",
        "\n",
        "    # Save the result to a new file\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"Data with stopwords removed saved to {output_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_csv_file = \"lemmatized_transcriptions.csv\"\n",
        "    output_csv_file = \"lemmatized_transcriptions_nostopwords.csv\"\n",
        "    remove_stopwords(input_csv_file, output_csv_file, \"tswana_stopwords.txt\", num_stopwords=100)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoiwXRVPye5u",
        "outputId": "553b5f54-2739-445b-9237-8e14e77c9231"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data with stopwords removed saved to lemmatized_transcriptions_nostopwords.csv\n"
          ]
        }
      ]
    }
  ]
}